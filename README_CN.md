# Ming-Lite-Omni

<p align="center">
    <img src="./figures/ant-bailing.png" width="100"/>
<p>

<p align="center">ğŸ“‘ <a href="https://github.com/inclusionAI/Ming">æŠ€æœ¯æŠ¥å‘Š</a>ï½œğŸ“–<a href="https://lucaria-academy.github.io/Ming-Omni/">é¡¹ç›®ä¸»é¡µ</a> ï½œğŸ¤— <a href="https://huggingface.co/inclusionAI/Ming-Lite-Omni">Hugging Faceé“¾æ¥</a>ï½œ ğŸ¤– <a href="https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni">ModelScopeé“¾æ¥</a>ï½œ


## å¼•è¨€

Ming-lite-omni æ˜¯ Ming-omni çš„è½»é‡çº§ç‰ˆæœ¬ï¼Œæºè‡ª Ling-liteï¼Œ28 äº¿æ¿€æ´»å‚æ•°ã€‚Ming-lite-omni æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘è¾“å…¥ï¼ŒåŒæ—¶åœ¨è¯­éŸ³å’Œå›¾åƒç”Ÿæˆæ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚Ming-lite-omni ä½¿ç”¨ä¸“ç”¨ç¼–ç å™¨ä»ä¸åŒæ¨¡æ€ä¸­æå– tokenï¼Œç„¶åç”± Lingï¼ˆé…å¤‡äº†æ–°å‹æ¨¡æ€ä¸“ç”¨è·¯ç”±çš„ MoE æ¶æ„ï¼‰è¿›è¡Œå¤„ç†ã€‚è¿™ç§è®¾è®¡ä½¿å•ä¸ªæ¨¡å‹èƒ½å¤Ÿåœ¨ç»Ÿä¸€çš„æ¡†æ¶å†…é«˜æ•ˆåœ°å¤„ç†å’Œèåˆå¤šæ¨¡æ€è¾“å…¥ï¼Œæ— éœ€å•ç‹¬æ„å»ºæ¨¡å‹ã€è¿›è¡Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒæˆ–é‡æ–°è®¾è®¡ç»“æ„ï¼Œä»è€Œç®€åŒ–äº†å„ç§ä»»åŠ¡çš„æ‰§è¡Œã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒMing-lite-omni è¶…è¶Šäº†ä¼ ç»Ÿçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ”¯æŒéŸ³é¢‘å’Œå›¾åƒç”Ÿæˆã€‚è¿™æ˜¯é€šè¿‡é›†æˆç”¨äºè‡ªç„¶è¯­éŸ³çš„é«˜çº§éŸ³é¢‘è§£ç å™¨å’Œç”¨äºé«˜è´¨é‡å›¾åƒç”Ÿæˆçš„ Ming-Lite-Uni å®ç°çš„ï¼Œä¹Ÿä½¿æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œæƒ…æ™¯æ„ŸçŸ¥èŠå¤©ã€æ‰§è¡Œæ–‡æœ¬åˆ°è¯­éŸ³åˆ°è½¬åŒ–ä»¥åŠå¤šç§å›¾åƒç¼–è¾‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMing-lite-omni ä¸ºè·¨æ‰€æœ‰æ¨¡æ€çš„ç»Ÿä¸€æ„ŸçŸ¥å’Œç”Ÿæˆæä¾›äº†å¼ºå¤§çš„è§£å†³æ–¹æ¡ˆã€‚
å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMing-lite-omni æ˜¯æˆ‘ä»¬æ‰€çŸ¥çš„ç¬¬ä¸€ä¸ªåœ¨æ¨¡æ€æ”¯æŒæ–¹é¢ä¸ GPT-4o åŒ¹æ•Œçš„å¼€æºæ¨¡å‹ï¼Œæˆ‘ä»¬å…¬å¼€æ‰€æœ‰ä»£ç å’Œæ¨¡å‹æƒé‡ï¼Œä»¥é¼“åŠ±ç¤¾åŒºçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå¼€å‘ã€‚

<p align="center">
    <img src="./figures/ming.png" width="800"/>
<p>

## ğŸ“Œ æ›´æ–°

* [2025.06.12] ğŸ”¥  [æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2506.09344) åœ¨arxivå‘å¸ƒã€‚
* [2025.05.28] ğŸ”¥  æ­£å¼ç‰ˆå‘å¸ƒï¼Œæ€§èƒ½æ›´ä½³ï¼Œæ”¯æŒå›¾åƒç”Ÿæˆã€‚
* [2025.05.04] ğŸ”¥ å‘å¸ƒMing-lite-omniæµ‹è¯•ç‰ˆæœ¬[Ming-lite-omni-Preview](https://github.com/inclusionAI/Ming/tree/Ming-Lite-Omni-Preview)ã€‚


## å…³é”®ç‰¹å¾

- **ç»Ÿä¸€å…¨æ¨¡æ€æ„ŸçŸ¥**: Ming-lite-omni å»ºç«‹åœ¨ [Ling](https://github.com/inclusionAI/Ling)ï¼ˆMoE æ¶æ„ LLMï¼‰ä¹‹ä¸Šï¼Œå®ƒå¯ä»¥è§£å†³ä»»åŠ¡å†²çªå¹¶ç¡®ä¿é€šè¿‡ç‰¹å®šæ¨¡æ€çš„è·¯ç”±å™¨å¯¹æ¥è‡ªä¸åŒæ¨¡æ€çš„ token è¿›è¡Œä¸€è‡´é›†æˆã€‚
- **ç»Ÿä¸€æ„ŸçŸ¥ä¸ç”Ÿæˆ**: Ming-lite-omni å®ç°ç»Ÿä¸€ç†è§£ä¸ç”Ÿæˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç†è§£å¤šæ¨¡æ€æŒ‡ä»¤å’Œç”¨æˆ·æ„å›¾ï¼Œä»è€Œæå‡ç”Ÿæˆè´¨é‡å¹¶æé«˜è·¨å¤šä»»åŠ¡çš„å¯ç”¨æ€§ã€‚
- **åˆ›æ–°çš„ç”Ÿæˆèƒ½åŠ›**: å¯ä»¥åŒæ—¶æ„ŸçŸ¥æ‰€æœ‰æ¨¡æ€å¹¶ç”Ÿæˆé«˜è´¨é‡æ–‡æœ¬ã€å®æ—¶è¯­éŸ³å’Œç”ŸåŠ¨å›¾åƒï¼Œåœ¨å›¾åƒæ„ŸçŸ¥ã€è§†å¬äº¤äº’å’Œå›¾åƒç”Ÿæˆç­‰å¤šç§ä»»åŠ¡ä¸­æä¾›å“è¶Šçš„è·¨æ¨¡æ€æ€§èƒ½ã€‚

##  è¯„æµ‹

Ming-lite-omni åœ¨å›¾åƒæ„ŸçŸ¥ã€è§†å¬äº¤äº’å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œå±•ç°å‡ºå“è¶Šçš„è·¨æ¨¡æ€æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨å›¾åƒæ„ŸçŸ¥ä»»åŠ¡ä¸­ï¼ŒMing-lite-omni ä»…æ¿€æ´» 2.8B ä¸ªå‚æ•°ï¼Œä¾¿è·å¾—äº†ä¸ Qwen2.5-VL-7B ç›¸å½“çš„æ€§èƒ½ã€‚å®ƒåœ¨ç«¯åˆ°ç«¯è¯­éŸ³ç†è§£å’ŒæŒ‡ä»¤è·Ÿéšæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº† Qwen2.5-Omni å’Œ Kimi-Audioã€‚å®ƒè¿˜æ”¯æŒåŸç”Ÿåˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€ç¼–è¾‘å’Œé£æ ¼è¿ç§»ï¼ŒGenEval å¾—åˆ† 0.64ï¼Œè¶…è¶Š SDXL ç­‰ä¸»æµæ¨¡å‹ã€‚åœ¨ FID æŒ‡æ ‡æ–¹é¢ï¼ŒMing-lite-omni è¾¾åˆ° 4.85ï¼Œè®¾å®šäº†ç°æœ‰æ–¹æ³•çš„æ–° SOTAã€‚


<p align="center">
    <img src="./figures/performance.png" width="800"/>
<p>


### å›¾åƒ benchmark
<div align="center">

| Benchmarks        | Ming-lite-omni |    Qwen2.5-VL-7B-Instruct    | InternVL2.5-8B-MPO |
|:------------------|:--------------:|:----------------------------:|:------------------:|
| AI2D              |      83.1      |             84.4             |    <b>84.5</b>     |
| HallusionBench    |  <b>55.0</b>   |             55.8             |        51.7        |
| MMBench_TEST_V11  |      80.8      |         <b>82.8</b>          |        82.0        |
| MMMU              |      56.3      |         <b>56.6</b>          |        54.8        |
| MMStar            |      64.7      |             65.3             |    <b>65.2</b>     |
| MMVet             |      71.3      |             71.6             |        68.1        |
| MathVista         |  <b>71.6</b>   |             68.1             |        67.9        |
| OCRBench          |  <b>88.4</b>   |             87.8             |        88.2        |
| Average           |      71.4      |         <b>71.5</b>          |        70.3        |

</div>


#### ç™¾ç§‘çŸ¥è¯† Benchmarks  
<div align="center">

| Object Recognition   | Ming-lite-omni |  Qwen2.5-VL-7B-Instruct  |
|:---------------------|:--------------:|:------------------------:|
| Plants               |   **54.96**    |           47.8           |
| Animals              |    **56.7**    |          50.85           |
| Vehicles             |     41.91      |        **42.29**         |
| Food & Ingredients   |   **62.28**    |          54.09           |
| Dishes               |    **44.3**    |          39.07           |
| General              |     91.08      |        **92.42**         |
| Average              |   **58.54**    |          54.43           |

</div>

### è§†é¢‘ benchmark

<div align="center">

| Benchmarks              | Ming-lite-omni | Qwen2.5VL-7B-Instruct |
|:------------------------|:--------------:|:---------------------:|
| VideoMME                |      67.0      |      <b>67.3</b>      |
| MVBench                 |      67.7      |      <b>67.4</b>      |
| Video-MMMU              |      46.3      |      <b>47.4</b>      |
| LongVideoBench          |      56.6      |         54.7          |
| Average                 |  <b>59.4</b>   |         59.2          |

</div>
æ³¨: æ‰€æœ‰æ¨¡å‹å‡åŸºäº 128 ä¸ªå‡åŒ€é‡‡æ ·çš„å¸§è¿›è¡Œè¯„ä¼°ã€‚

### éŸ³é¢‘ benchmark
#### SpeechQA

<div align="center">

| Model            |    Average    | AlpacaEval  | CommonEval  |    SD-QA     |     MMSU     |  OpenBookQA  |    IFEval    |   AdvBench    |
|:-----------------|:-------------:|:-----------:|:-----------:|:------------:|:------------:|:------------:|:------------:|:-------------:|
| Qwen2-Audio-chat |     3.545     |    3.69     |    3.40     |    35.35     |    35.43     |    49.01     |    22.57     |     98.85     |
| Baichuan-Audio   |     3.695     |    4.00     |    3.39     |    49.64     |    48.80     |    63.30     |    41.32     |     86.73     |
| GLM-4-Voice      |     3.77      |    4.06     |    3.48     |    43.31     |    40.11     |    52.97     |    24.91     |     88.08     |
| Kimi-Audio       |     4.215     |    4.46     |    3.97     | <b>63.12</b> | <b>62.17</b> | <b>83.52</b> | <b>61.10</b> | <b>100.00</b> |
| Qwen2.5-Omni     |     4.21      |    4.49     |    3.93     |    55.71     |    61.32     |    81.10     |    52.87     |     99.42     |
| Ming-lite-omni   |  <b>4.34</b>  | <b>4.63</b> | <b>4.06</b> |    58.84     |    47.53     |    61.98     |    58.36     |     99.04     |
</div>

#### ASR

<div align="center">

|     Model      | aishell1 | aishell2_android | aishell2_ios | cv15_zh  | fleurs_zh | wenetspeech_meeting | wenetspeech_net | librispeech_test_clean | librispeech_test_other | multilingual_librispeech | cv15_en  | fleurs_en |  voxpopuli_v1.0_en   |
|:--------------:|:--------:|:----------------:|:------------:|:--------:|:---------:|:-------------------:|:---------------:|:----------------------:|:----------------------:|:------------------------:|:--------:|:---------:|:--------------------:|
| Ming-lite-omni |   1.47   |     **2.55**     |   **2.52**   |   6.31   |   2.96    |        5.95         |      5.46       |          1.44          |          2.80          |         **4.15**         | **6.89** | **3.39**  |       **5.80**       |
|  Qwen2.-Omni   |   1.18   |       2.75       |     2.63     | **5.20** |   3.00    |      **5.90**       |      7.70       |          1.80          |          3.40          |           7.56           |   7.60   |   4.10    |       **5.80**       |
|  Qwen2-Audio   |   1.53   |       2.92       |     2.92     |   6.90   |   7.50    |        7.16         |      8.42       |          1.60          |          3.60          |           5.40           |   8.60   |   6.90    |         6.84         |
|   Kimi-Audio   | **0.60** |       2.64       |     2.56     |   7.21   | **2.69**  |        6.28         |    **5.37**     |        **1.28**        |        **2.42**        |           5.88           |  10.31   |   4.44    |         7.97         |

</div>



### ä¿¡æ¯æ£€ç´¢ Benchmark
<div align="center">

| Model          | InfoSeek_H-mean | InfoSeek_unseen_question | InfoSeek_unseen_entity |
|:---------------|:---------------:|:------------------------:|:----------------------:|
| GPT-4o         |  <b>36.05</b>   |            -             |           -            |
| PaLI-X         |      22.06      |           23.5           |          20.8          |
| Qwen2.5-vl-32B |      19.35      |          20.55           |         18.28          |
| Ming-lite-omni |      27.7       |         **30.4**         |        **25.4**        |
</div>



### OCR
<div align="center">

| Model              | Ming-lite-omni | Qwen2.5-VL-7B-Instruct  |
|:-------------------|:--------------:|:-----------------------:|
| ChartQA_TEST       |      85.1      |       <b>87.3</b>       |
| DocVQA_TEST        |       93       |       <b>95.7</b>       |
| OCRBenchV2_en/zh   |    53.3/52     |    <b>56.3/57.2</b>     |
| OmniDocBenchâ†“      | 34/<b>34.4</b> |    <b>30.8</b>/39.8     |
| TextVQA_VAL        |      82.8      |       <b>84.9</b>       |
</div>

### GUI
<div align="center">

| Model                      | Ming-lite-omni | InternVL3 8B | Qwen2.5-VL-7B-Instruct | 
|:---------------------------|:--------------:|:------------:|:----------------------:|
| ScreenSpot                 |  <b>82.1</b>   |     79.5     |         78.9*          |
| ScreenSpot-V2              |  <b>84.1</b>   |     81.4     |           -            |
| AITZ(EM)                   |  <b>66.6</b>   |      -       |         57.6*          |
</div>
æ³¨: * è¡¨ç¤ºå¤ç°çš„ç»“æœ.



### ç»Ÿä¸€ç”Ÿæˆ Benchmark

<div align="center">

| Model          | single_object | two_object |  counting  |  colors  | position | color_attr | GENEVAL  | DPGBench  |     FIDâ†“      |
|:---------------|:-------------:|:----------:|:----------:|:--------:|:--------:|:----------:|:--------:|:---------:|:-------------:|
| Ming-lite-omni |  **0.9875**   | **0.7727** | **0.6812** |  0.7872  |   0.31   |    0.29    | **0.64** |   81.72   |   **4.85**    |
| Metaquery-XL   |       -       |     -      |     -      |    -     |    -     |     -      |   0.61   | **82.05** |     6.02      |
| SDv2.1         |     0.98      |    0.51    |    0.44    | **0.85** |   0.07   |    0.17    |   0.50   |   68.09   |     26.96     |
| Emu3-Gen       |     0.98      |    0.71    |    0.34    |   0.81   |   0.17   |    0.21    |   0.54   |   80.60   |       -       |
| SDXL           |     0.98      |    0.74    |    0.39    | **0.85** |   0.15   |    0.23    |   0.55   |   74.65   |     8.76      |
| Janus          |     0.97      |    0.68    |    0.30    |   0.84   | **0.46** |  **0.42**  |   0.61   |   79.68   |     10.10     |
| JanusFlow      |       -       |     -      |     -      |    -     |    -     |     -      |   0.63   |   80.09   |     9.51      |

</div>

è¯·å‚é˜…æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šä»¥è·å¾—æ›´å…¨é¢çš„è¯„æµ‹ç»“æœã€‚


## æ¨¡å‹ä¸‹è½½

å¯ä»¥é€šè¿‡Huggingfaceå’ŒModelScopeè¿›è¡Œä¸‹è½½ã€‚
<div align="center">

|     **æ¨¡å‹**     |  **è¾“å…¥æ¨¡æ€**   | **è¾“å‡ºæ¨¡æ€** |                                                                       **ä¸‹è½½é“¾æ¥**                                                                       |
|:--------------:|:-----------:|:--------:|:----------------------------------------------------------------------------------------------------------------------------------------------------:|
| Ming-Lite-Omni | å›¾ç‰‡,æ–‡æœ¬,è§†é¢‘,éŸ³é¢‘ | å›¾ç‰‡,æ–‡æœ¬,éŸ³é¢‘ | [ğŸ¤— HuggingFace](https://huggingface.co/inclusionAI/Ming-Lite-Omni) <br>[ğŸ¤– ModelScope](https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni) |
</div>
å¦‚æœæ‚¨çš„æ‰€åœ¨åœ°ä½äºä¸­å›½å¤§é™†ï¼Œå¼ºçƒˆå»ºè®®æ‚¨ä»ğŸ¤– <a href="https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni">ModelScope</a>ä¸‹è½½ã€‚

```shell
# huggingface
cd ./path/to/local/model
git lfs install
git clone https://huggingface.co/inclusionAI/Ming-Lite-Omni

# modelscope
cd ./path/to/local/model
pip install modelscope
modelscope download --model inclusionAI/Ming-Lite-Omni --local_dir ./
```

## æ¡ˆä¾‹
æ›´å¤šæ¡ˆä¾‹å‚è€ƒæˆ‘ä»¬çš„é¡¹ç›®ä¸»é¡µ [page](https://lucaria-academy.github.io/Ming-Omni/)ã€‚

## ä½¿ç”¨ç¤ºä¾‹

è¯·æŒ‰ç…§æ¨¡å‹[ä¸‹è½½é¡µé¢](#model-downloads)ä¸‹è½½æˆ‘ä»¬çš„æ¨¡å‹ï¼Œç„¶åæ‚¨å¯ä»¥å‚è€ƒä»¥ä¸‹ä»£ç è¿è¡Œ Ming-lite-omni æ¨¡å‹ã€‚

Python ç¯å¢ƒä¾èµ–é¡¹å®‰è£…ã€‚

```shell
pip install -r requirements.txt
pip install data/matcha_tts-0.0.5.1-cp38-cp38-linux_x86_64.whl
pip install diffusers==0.33.0
pip install nvidia-cublas-cu12==12.4.5.8  # for H20
```

æ³¨ï¼šä»¥ä¸‹ç¤ºä¾‹åœ¨ NVIDIA H800-80GB & CUDA 12.2ç‰ˆæœ¬é€šè¿‡è¿è¡Œæµ‹è¯•ã€‚åŠ è½½ bfloat16 æ ¼å¼çš„inclusionAI/Ming-Lite-Omni å¤§çº¦éœ€è¦ 40890MB å†…å­˜ã€‚

```python
import os
import torch
from transformers import AutoProcessor, GenerationConfig
from modeling_bailingmm import BailingMMNativeForConditionalGeneration

# build model
model = BailingMMNativeForConditionalGeneration.from_pretrained(
    "inclusionAI/Ming-Lite-Omni",
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True
).to("cuda")

assets_path = YOUR_ASSETS_PATH

# build processor
processor = AutoProcessor.from_pretrained("inclusionAI/Ming-Lite-Omni", trust_remote_code=True)
```

```python
# qa
messages = [
    {
        "role": "HUMAN",
        "content": [
            {"type": "text", "text": "è¯·è¯¦ç»†ä»‹ç»é¹¦é¹‰çš„ç”Ÿæ´»ä¹ æ€§ã€‚"}
        ],
    },
]
# Output:

# é¹¦é¹‰æ˜¯ä¸€ç§éå¸¸èªæ˜å’Œç¤¾äº¤æ€§å¼ºçš„é¸Ÿç±»ï¼Œå®ƒä»¬çš„ç”Ÿæ´»ä¹ æ€§éå¸¸ä¸°å¯Œå’Œæœ‰è¶£ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³äºé¹¦é¹‰ç”Ÿæ´»ä¹ æ€§çš„è¯¦ç»†ä»‹ç»ï¼š
# ### 1. **æ –æ¯åœ°**
# é¹¦é¹‰ä¸»è¦åˆ†å¸ƒåœ¨çƒ­å¸¦å’Œäºšçƒ­å¸¦åœ°åŒºï¼ŒåŒ…æ‹¬éæ´²ã€äºšæ´²ã€æ¾³å¤§åˆ©äºšå’Œå—ç¾æ´²ã€‚å®ƒä»¬é€šå¸¸ç”Ÿæ´»åœ¨æ£®æ—ã€è‰åŸã€æ²™æ¼ å’ŒåŸå¸‚ç¯å¢ƒä¸­ã€‚ä¸åŒç§ç±»çš„é¹¦é¹‰å¯¹æ –æ¯åœ°çš„è¦æ±‚æœ‰æ‰€ä¸åŒï¼Œä½†å¤§å¤šæ•°é¹¦é¹‰å–œæ¬¢æœ‰ä¸°å¯Œæ¤è¢«å’Œæ°´æºçš„åœ°æ–¹ã€‚
# ### 2. **é¥®é£Ÿ**
# é¹¦é¹‰æ˜¯æ‚é£Ÿæ€§åŠ¨ç‰©ï¼Œå®ƒä»¬çš„é¥®é£Ÿéå¸¸å¤šæ ·åŒ–ã€‚å®ƒä»¬çš„é£Ÿç‰©åŒ…æ‹¬ç§å­ã€åšæœã€æ°´æœã€è”¬èœã€èŠ±èœœå’Œæ˜†è™«ã€‚é¹¦é¹‰çš„å–™éå¸¸å¼ºå£®ï¼Œèƒ½å¤Ÿè½»æ¾åœ°æ‰“å¼€åšç¡¬çš„æœå£³å’Œåšæœã€‚ä¸€äº›é¹¦é¹‰è¿˜ä¼šåƒæ³¥åœŸæˆ–æ²™å­ï¼Œä»¥å¸®åŠ©æ¶ˆåŒ–å’Œè¡¥å……çŸ¿ç‰©è´¨ã€‚
# ......

```

```python
# image qa
messages = [
    {
        "role": "HUMAN",
        "content": [
            {"type": "image", "image": os.path.join(assets_path, "flowers.jpg")},
            {"type": "text", "text": "What kind of flower is this?"},
        ],
    },
]
# Output:

# The flowers in this image are forget-me-nots. These delicate blooms are known for their small, five-petaled flowers that come in various shades of blue, pink, and white. 
```

ä¸ºäº†èƒ½å¤Ÿè®©æ¨¡å‹åœ¨å›ç­”ä¹‹å‰æ€è€ƒï¼Œè¯·åœ¨é—®é¢˜å‰æ·»åŠ ä»¥ä¸‹ç³»ç»Ÿæç¤ºï¼š

```python
cot_prompt = "SYSTEM: You are a helpful assistant. When the user asks a question, your response must include two parts: first, the reasoning process enclosed in <thinking>...</thinking> tags, then the final answer enclosed in <answer>...</answer> tags. The critical answer or key result should be placed within \\boxed{}.\n"
# And your input message should be like this:
messages = [
    {
        "role": "HUMAN",
        "content": [
            {"type": "image", "image": os.path.join(assets_path, "reasoning.png")},
            {"type": "text", "text": cot_prompt + "In the rectangle $A B C D$ pictured, $M_{1}$ is the midpoint of $D C, M_{2}$ the midpoint of $A M_{1}, M_{3}$ the midpoint of $B M_{2}$ and $M_{4}$ the midpoint of $C M_{3}$. Determine the ratio of the area of the quadrilateral $M_{1} M_{2} M_{3} M_{4}$ to the area of the rectangle $A B C D$.\nChoices:\n(A) $\frac{7}{16}$\n(B) $\frac{3}{16}$\n(C) $\frac{7}{32}$\n(D) $\frac{9}{32}$\n(E) $\frac{1}{5}$"},
        ],
    },
]
# Output:
# \<think\>\nOkay, so I have this problem about a rectangle ABCD ... (thinking process omitted) ... So, the correct answer is C.\n\</think\>\n\<answer\>\\boxed{C}\</answer\>\n\n
```

```python
# video qa
messages = [
    {
        "role": "HUMAN",
        "content": [
            {"type": "video", "video": os.path.join(assets_path, "yoga.mp4")},
            {"type": "text", "text": "What is the woman doing?"},
        ],
    },
]
# Output:

# The image shows a woman performing a yoga pose on a rooftop. She's in a dynamic yoga pose, with her arms and legs extended in various positions.

```

```python
# multi-turn chat
messages = [
    {
        "role": "HUMAN",
        "content": [
            {"type": "text", "text": "ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ"},
        ],
    },
    {
        "role": "ASSISTANT",
        "content": [
            {"type": "text", "text": "åŒ—äº¬"},
        ],
    },
    {
        "role": "HUMAN",
        "content": [
            {"type": "text", "text": "å®ƒçš„å åœ°é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿæœ‰å¤šå°‘å¸¸ä½äººå£ï¼Ÿ"},
        ],
    },
]
# Output:

# åŒ—äº¬å¸‚çš„æ€»é¢ç§¯çº¦ä¸º16,410.54å¹³æ–¹å…¬é‡Œï¼Œå¸¸ä½äººå£çº¦ä¸º21,542,000äººã€‚
```

```python
# Preparation for inference
text = processor.apply_chat_template(messages, add_generation_prompt=True)
image_inputs, video_inputs, audio_inputs = processor.process_vision_info(messages)
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    audios=audio_inputs,
    return_tensors="pt",
)
inputs = inputs.to(model.device)
for k in inputs.keys():
    if k == "pixel_values" or k == "pixel_values_videos" or k == "audio_feats":
        inputs[k] = inputs[k].to(dtype=torch.bfloat16)

# call generate
generation_config = GenerationConfig.from_dict({'no_repeat_ngram_size': 10})
generated_ids = model.generate(
    **inputs,
    max_new_tokens=512,
    use_cache=True,
    eos_token_id=processor.gen_terminator,
    generation_config=generation_config,
)
generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)[0]
print(output_text)
```

### éŸ³é¢‘ä»»åŠ¡

```python
# ASR
messages = [
    {
        "role": "HUMAN",
        "content": [
            {"type": "text", "text": "Please recognize the language of this speech and transcribe it. Format: oral."},
            {"type": "audio", "audio": 'data/wavs/BAC009S0915W0292.wav'},
        ],
    },
]
# we use whisper encoder for ASR task, so need modify code above
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    audios=audio_inputs,
    return_tensors="pt",
    audio_kwargs={'use_whisper_encoder': True}
)

outputs = model.generate(
    **inputs,
    max_new_tokens=512,
    use_cache=True,
    eos_token_id=processor.gen_terminator,
    generation_config=generation_config,
    use_whisper_encoder=True
)

```

```python
# speech2speech
messages = [
    {
        "role": "HUMAN",
        "content": [
            {"type": "audio", "audio": 'data/wavs/speechQA_sample.wav'},
        ],
    },
]
generation_config = GenerationConfig.from_dict({
    'output_hidden_states': True,
    'return_dict_in_generate': True,
    'no_repeat_ngram_size': 10}
)

outputs = model.generate(
    **inputs,
    max_new_tokens=512,
    use_cache=True,
    eos_token_id=processor.gen_terminator,
    generation_config=generation_config,
    use_whisper_encoder=False
)

generated_ids = outputs.sequences
generated_ids_trimmed = [
    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]

# speechQA result
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)[0]

# for TTS
from modeling_bailing_talker import AudioDetokenizer

model_name_or_path = model.config._name_or_path
audio_detokenizer = AudioDetokenizer(
    f'{model_name_or_path}/talker/audio_detokenizer.yaml',
    flow_model_path=f'{model_name_or_path}/talker/flow.pt',
    hifigan_model_path=f'{model_name_or_path}/talker/hift.pt'
)
spk_input = torch.load('data/spks/luna.pt')
thinker_reply_part = outputs.hidden_states[0][0] + outputs.hidden_states[0][-1]
# Setting thinker_reply_part to None allows the talker to operate as a standalone TTS model, independent of the language model.
audio_tokens = model.talker.omni_audio_generation(
    output_text, 
    thinker_reply_part=thinker_reply_part, **spk_input)
waveform = audio_detokenizer.token2wav(audio_tokens, save_path='out.wav', **spk_input)

```
æ›´å¤šå…³äº ASR, SpeechQA, å’Œ TTS ä»»åŠ¡çš„ç»†èŠ‚ï¼Œå‚è€ƒ`test_audio_tasks.py`ã€‚

### å›¾åƒç”Ÿæˆ&ç¼–è¾‘

Ming-omni åŸç”Ÿæ”¯æŒå›¾ç‰‡ç”Ÿæˆå’Œå›¾ç‰‡ç¼–è¾‘ï¼Œåªéœ€è¦åœ¨generateå‡½æ•°ä¸­æ·»åŠ ç›¸åº”çš„å‚æ•°å³å¯ä½¿ç”¨è¯¥åŠŸèƒ½ã€‚


```python
# Image generation mode currently limits the range of input pixels.
gen_input_pixels = 451584
processor.max_pixels = gen_input_pixels
processor.min_pixels = gen_input_pixels

def generate(messages, processor, model, **image_gen_param):
    text = processor.apply_chat_template(messages, add_generation_prompt=True)
    image_inputs, video_inputs, audio_inputs = processor.process_vision_info(messages)

    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        audios=audio_inputs,
        return_tensors="pt",
    ).to(model.device)

    for k in inputs.keys():
        if k == "pixel_values" or k == "pixel_values_videos" or k == "audio_feats":
            inputs[k] = inputs[k].to(dtype=torch.bfloat16)
    
    print(image_gen_param)
    image = model.generate(
        **inputs,
        image_gen=True,
        **image_gen_param,
    )
    return image

```

Text-to-image
```python
messages = [
    {
        "role": "HUMAN",
        "content": [
            {"type": "text", "text": "Draw a girl with short hair."},
        ],
    }
]
image = generate(
   messages=messages, processor=processor, model=model, 
   image_gen_cfg=6.0, image_gen_steps=20, image_gen_width=480, image_gen_height=544
)
image.save("./t2i.jpg")
```

Edit
```python
messages = [
    {
        "role": "HUMAN",
        "content": [
            {"type": "image", "image": "samples/cake.jpg"},
            {"type": "text", "text": "add a candle on top of the cake"},
        ],
    }
]
image = generate(
   messages=messages, processor=processor, model=model, 
   image_gen_cfg=6.0, image_gen_steps=20, image_gen_width=512, image_gen_height=512
)
image.save("./edit.jpg")
```


## è®¸å¯å’Œæ³•å¾‹å…è´£å£°æ˜

æœ¬ä»£ç ä»“åº“éµå¾ª [MIT License](./LICENSE), æ³•å¾‹å…è´£å£°æ˜[LEGAL.md](./LEGAL.md) ä½äºé¡¹ç›®æ ¹ç›®å½•ä¸‹ã€‚


## å¼•ç”¨

å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰æ‰€å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ã€‚

```bibtex

@misc{Mingomni2025,
      title  = {Ming-Omni: A Unified Multimodal Model for Perception and Generation}, 
      author = {Inclusion AI},
      year = {2025},
      eprint = {2506.09344},
      archivePrefix = {arXiv},
      url = {https://arxiv.org/abs/2506.09344}
}
```
